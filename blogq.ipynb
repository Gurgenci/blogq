{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q & A on my Substack Blog #\n",
    "* This notebook can be used to query my blog posts\n",
    "* The Large Language Model (LLM) is `llama 3.1`\n",
    "* I am accessing the LLM **ollama** and **LLama3** and **langchain**\n",
    "* My starting reference was  https://www.datacamp.com/tutorial/llama-3-1-rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "Make sure that we are running in a `venv`. Then install all the packages seen in the following cell.  I comment the line out after the package is installed so that VS Code will not try to reinstall the package the next time I run the notebook.  There is no harm in trying to reinstall a package already installed but it wastes time..\n",
    "\n",
    "The first time you run this notebook on your computer, uncomment all lines in the following cell and run only the following cell.  It will `pip install` the required python packages used in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langchain_community\n",
    "#!pip3 install bs4\n",
    "#!pip3 install chromadb\n",
    "# !pip3 install matplotlib\n",
    "# !ollama pull nomic-embed-text\n",
    "# !pip3 install langchain\n",
    "# !pip3 install scikit-learn\n",
    "# !pip3 install langchain-ollama\n",
    "# !pip3 install pandas\n",
    "# !pip3 install pyarrow\n",
    "#\n",
    "#\n",
    "# !pip3 install --upgrade langchain langchain-community pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function `md()`\n",
    "I like using the markdown function **md** instead of **print**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def md(s):\n",
    "    display(Markdown(s))\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities ##\n",
    "The following cell include the basic functions and parameters.\n",
    "\n",
    "**PARAMETERS**\n",
    "* `SEP` This is the separator string I use in my blog post pages.  It separates a post into sections with each having their own individual content.  Each blog post will typically have 3 to 5 sections.  All sections from all posts are stored as the `Doc_List`.   \n",
    "* `EMBEDDER` I use a quick embedder. The **llama3** embedding takes several hours.  The **nomic-embed-text** takes several minutes.\n",
    "* `LLMNAME` **llama3**\n",
    "  \n",
    "**FUNCTIONS**\n",
    "* `sepstr` Separates a string (typically the entire cotents of one post) into congruent sections using the `SEP` as the separator.  I remove the starting section (the part on the blog post before the furst occurrence of the `SEP`)\n",
    "* I access `0llama` using `langchain`.  Using `langchain`  is not necessary.  It is just a convenience.\n",
    "* `embedstr` creates the embedding vector for a string using `EMBEDDER` as default\n",
    "* `docs2vectors` takes an array of documents and generates the vector store `varr`.  **IMPORTANT** This function uses the embedder in `Ollama` directly  not through my `embedstr`. Therefore, make sure that both calls use the same `EMBEDDER`.\n",
    "* `cosine_similarity` The distance between the two embedding vectors\n",
    "* `most_similar` finds the `n` vectors in the vector store `varr` that are closest to the argument vector.  Returns the indices of those documents.  **IMPORTANT** Make sure that `varr[i]` is the embedding vector for `Doc_List[i]` all the time.  Erase the vector store file from the local disk when you add new post titles to the post directory `listfilename`.  This file is a file I keep in my Google drive with public access.  Download it to your local folder and use it fro that folder if you want to change it.  Otherwise, you may keep using it off my google drive address.\n",
    "* `save_vectorstore` saves the vectors to the file VECTORSTORE.  The next time yu run this notebook, new embeddings will not be created but will be read from the file VECTORSTORE.  Delete VECTORSTORE manually to regenerate the embeddings.\n",
    "* `load_vectorstore` Loads the embedding vectors from the file.  it does not check if they are the right ones for the documents read from `listfilename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "SEP=\"-+-+-+-+\"\n",
    "EMBEDDER=\"nomic-embed-text\"\n",
    "EMBEDDER=\"llama3\"\n",
    "VECTORSTORE=\"data/\"+EMBEDDER+\".pkl\"\n",
    "LLMNAME=\"llama3\"\n",
    "LOGFILE=\"\"\n",
    "\n",
    "# Open log file\n",
    "# The following function reads the list of files in the \"log\" folder\n",
    "# These files are named as \"log1.txt\", \"log2.txt\", ...\n",
    "# Ignore all other files in the folder\n",
    "# The function determines the next file name and opens it in write mode\n",
    "def open_log():\n",
    "    global LOGFILE\n",
    "    # Read the list of files in the \"log\" folder\n",
    "    import os\n",
    "    files = os.listdir(\"log\")\n",
    "    # Determine the next file name\n",
    "    files = [f for f in files if f.endswith('.md')]\n",
    "    if not files:\n",
    "        next_file = \"log1.md\"\n",
    "    else:\n",
    "        next_file = \"log\" + str(max([int(f[3:-3]) for f in files]) + 1) + \".md\"\n",
    "    # Open the file in write mode\n",
    "    LOGFILE = open(\"log/\" + next_file, \"w\")\n",
    "    from datetime import datetime\n",
    "    LOGFILE.write(\"Date and time: \" + str(datetime.now()) + \"\\n\\n\")\n",
    "    return LOGFILE\n",
    "\n",
    "# The following function closes the log file\n",
    "def close_log():\n",
    "    LOGFILE.close()\n",
    "\n",
    "# The following function writes a string to the log file\n",
    "def log2file(s):\n",
    "    LOGFILE.write(s + \"\\n\")\n",
    "    LOGFILE.flush()\n",
    "\n",
    "#\n",
    "# The following function takes a text string `text` and separates it to a list\n",
    "# of strings using the separator `sep`\n",
    "def sepstr(text, sep=SEP, remove=1):\n",
    "    # Split the text to a list of strings\n",
    "    lst=text.split(sep)\n",
    "    # Remove the empty strings\n",
    "    lst=[s for s in lst if s]\n",
    "    # Remove the first string\n",
    "    if remove>0:\n",
    "        lst=lst[remove:]\n",
    "    return lst\n",
    "\n",
    "# I access 0llama using langchain.  Using langchain  is not necessary.  It is just a convenience.\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "LLM = Ollama(model=LLMNAME)\n",
    "\n",
    "# The following function takes a text string and returns its embedding\n",
    "#  The second argument is the name of the embedder to be used\n",
    "def embedstr(text, embedder=EMBEDDER):\n",
    "    # Initialize the Ollama embeddings\n",
    "    embeddings = OllamaEmbeddings(model=embedder)\n",
    "    # Embed the text\n",
    "    embedding = embeddings.embed_query(text)\n",
    "    return embedding\n",
    "\n",
    "# The following function takes a list of documents and returns a list of embeddings\n",
    "# The second argument is the name of the embedder to be used\n",
    "def docs2vectors(docs, embedder=EMBEDDER):\n",
    "    # Initialize the Ollama embeddings\n",
    "    embeddings = OllamaEmbeddings(model=embedder)\n",
    "    # Embed the documents\n",
    "    varr = embeddings.embed_documents(docs)\n",
    "    return varr\n",
    "\n",
    "# The following function returns the distance between two embeddings\n",
    "# The distance is calculated as the cosine similarity between the two embeddings\n",
    "# It is normalized to be between 0 and +/- 1\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# The following function returns the indices of the top `n` most similar embeddings to the given embedding\n",
    "# The `v` is the embedding and the `varr` is the array of embeddings\n",
    "def most_similar(v, varr, n=5):\n",
    "    # Calculate the cosine similarity between the given embedding and all the embeddings in the array\n",
    "    sims = np.array([cosine_similarity(v, v2) for v2 in varr])\n",
    "    # Get the indices of the top `n` most similar embeddings\n",
    "    indices = np.argsort(sims)[::-1][:n]\n",
    "    return indices, sims[indices]\n",
    "\n",
    "# The following runction returns the similarity between two texts using the EMBEDDER\n",
    "# def similarity(text1, text2, embedder=EMBEDDER):\n",
    "#     # Embed the two texts\n",
    "#     v1 = embedstr(text1, embedder)\n",
    "#     v2 = embedstr(text2, embedder)\n",
    "#     # Calculate the cosine similarity between the two embeddings\n",
    "#     return cosine_similarity(v1, v2)\n",
    "\n",
    "# # The following function returns the start of the string s1 inthe string s2:\n",
    "# def startof(s1, s2):\n",
    "#     return s2.find(s1)\n",
    "\n",
    "# Vector store\n",
    "import pickle\n",
    "# The following function is used to save the vectorstore to a file\n",
    "# The `vectorstore` is the array of embeddings and the `file_path` is the path to save the file\n",
    "def save_vectorstore(vectorstore, file_path=VECTORSTORE):\n",
    "# Open the file in write-binary mode and save the vectorstore\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(vectorstore, f)\n",
    "\n",
    "# The following function is used to load the vectorstore from a file\n",
    "# The `file_path` is the path to load the file\n",
    "def load_vectorstore(file_path=VECTORSTORE):\n",
    "    # Open the file in read-binary mode and load the vectorstore\n",
    "    with open(file_path, 'rb') as f:\n",
    "        vectorstore = pickle.load(f)\n",
    "    return vectorstore\n",
    "\n",
    "# The following function returns True if the vectorstore file exists\n",
    "def vectorstore_exists(file_path=VECTORSTORE):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert web page contents to embedding vectors:\n",
    "`read_page_list` Read the list of the page URLs and page titles\n",
    "\n",
    "`Docs_List` A Document array to hold the page contents\n",
    "\n",
    "`Varr` Embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://halimgur.substack.com/p/imaginary-conversation-reflections Imaginary Conversation: Reflections on Israel and the Middle East\n",
      "https://halimgur.substack.com/p/the-unbearable-fakeness-of-politics The Unbearable Fakeness of Politics: A Foreign Perspective on the 2024 US Election\n",
      "...\n",
      "Retirement: A Journey of Continued Relevance and Learning https://halimgur.substack.com/p/retirement-a-journey-of-continued\n",
      "Finished reading 57 page addresses\n",
      "Now load them one by one\n",
      "Finished loading 57 documents\n",
      "Now split them using the separator -+-+-+-+\n",
      "https://halimgur.substack.com/p/imaginary-conversation-reflectionshttps://halimgur.substack.com/p/the-unbearable-fakeness-of-politicshttps://halimgur.substack.com/p/living-in-a-simulation-how-real-ishttps://halimgur.substack.com/p/the-ai-reduxhttps://halimgur.substack.com/p/join-my-new-subscriber-chat-on-ourhttps://halimgur.substack.com/p/when-will-you-diehttps://halimgur.substack.com/p/the-art-of-restraint-why-the-parishttps://halimgur.substack.com/p/remembrance-of-brilliance-past-andhttps://halimgur.substack.com/p/nuclear-power-no-fix-for-our-brokenhttps://halimgur.substack.com/p/sadness-of-a-ghost-townhttps://halimgur.substack.com/p/the-best-summer-holiday-spot-in-turkeyhttps://halimgur.substack.com/p/class-reunionhttps://halimgur.substack.com/p/turk-fatih-tutakhttps://halimgur.substack.com/p/back-in-turkey-after-four-years-parthttps://halimgur.substack.com/p/back-in-turkey-after-four-years-weekhttps://halimgur.substack.com/p/turkish-republic-the-age-of-maturityhttps://halimgur.substack.com/p/to-understand-the-worldhttps://halimgur.substack.com/p/beyond-the-echo-chambers-a-deeperhttps://halimgur.substack.com/p/too-much-of-a-good-thinghttps://halimgur.substack.com/p/a-sinn-fein-member-became-the-nexthttps://halimgur.substack.com/p/a-cut-and-paste-operationhttps://halimgur.substack.com/p/retirement-a-journey-of-continuedhttps://halimgur.substack.com/p/more-missed-opportunities-in-arabhttps://halimgur.substack.com/p/the-requiem-for-a-dream-israels-untakenhttps://halimgur.substack.com/p/the-importance-of-elite-consensushttps://halimgur.substack.com/p/openai-astounds-us-againhttps://halimgur.substack.com/p/why-did-elon-musk-buy-twitterhttps://halimgur.substack.com/p/elon-musk-the-spacemanhttps://halimgur.substack.com/p/let-us-talk-about-elon-muskhttps://halimgur.substack.com/p/despicable-acts-part-2https://halimgur.substack.com/p/despicable-deedshttps://halimgur.substack.com/p/rogue-age-and-climate-change-unpredictablehttps://halimgur.substack.com/p/rogue-age-accessory-1-populationhttps://halimgur.substack.com/p/rogue-renaissance-on-globe-with-upheavalshttps://halimgur.substack.com/p/the-great-stagnation-ends-but-forhttps://halimgur.substack.com/p/when-the-rivers-run-dryhttps://halimgur.substack.com/p/the-voice-referendum-in-australiahttps://halimgur.substack.com/p/conspiracy-theories-part-2https://halimgur.substack.com/p/conspiracy-theories-part-1https://halimgur.substack.com/p/how-many-more-ruins-in-great-britainhttps://halimgur.substack.com/p/while-watching-utopia-on-abchttps://halimgur.substack.com/p/lying-oracles-and-the-anyone-andhttps://halimgur.substack.com/p/the-one-thing-necessary-for-the-triumphhttps://halimgur.substack.com/p/will-there-be-a-warhttps://halimgur.substack.com/p/how-do-we-select-our-informationhttps://halimgur.substack.com/p/do-we-know-why-we-know-what-we-knowhttps://halimgur.substack.com/p/does-layering-affect-heat-losshttps://halimgur.substack.com/p/elections-in-turkeyhttps://halimgur.substack.com/p/monster-wave-soon-to-hit-your-shorehttps://halimgur.substack.com/p/try-counting-carbons-not-calorieshttps://halimgur.substack.com/p/bridge-fixture-for-five-playershttps://halimgur.substack.com/p/the-attainment-of-happinesshttps://halimgur.substack.com/p/competent-intelligence-is-here-willhttps://halimgur.substack.com/p/the-prodigal-bird-returnshttps://halimgur.substack.com/p/earthquake-thoughts-and-factshttps://halimgur.substack.com/p/why-were-so-many-buildings-destroyedhttps://halimgur.substack.com/p/retirement-a-journey-of-continuedFinished splitting 57 documents to 411\n",
      "Creating vectorstore\n",
      "Created and saved vectorstore\n"
     ]
    }
   ],
   "source": [
    "def readfromdrive():\n",
    "    import requests\n",
    "    listfilename=\"https://drive.google.com/file/d/1LA9LV9WyvoRZgHL8R7DGFbFzfTAMcKbC/view?usp=drive_link\"\n",
    "    # Modify the URL to make it a direct download link\n",
    "    file_id = listfilename.split('/d/')[1].split('/')[0]\n",
    "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "    response = requests.get(download_url)\n",
    "    return response.text\n",
    "def readlocal():\n",
    "    with open(\"data/general_textlist.txt\", \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# The following function reads a text file from my Google Drive.  The file is organised as follows:\n",
    "# 1. The first line is the first URL\n",
    "# 2. The second line is the post title on that URL\n",
    "# 3. The third line is the second URL\n",
    "# 4. The fourth line is the post title on that URL\n",
    "# 5. etc.\n",
    "# The function returns two lists: the first list contains the URLs and the second list contains the titles.\n",
    "def read_page_list():\n",
    "    text=readlocal()\n",
    "    lines = text.split('\\n')\n",
    "    page_list=[]\n",
    "    title_list=[]\n",
    "    for i in range(0,len(lines),2):\n",
    "        page_list.append(lines[i].strip())\n",
    "        title_list.append(lines[i+1].strip())\n",
    "        if i<3:\n",
    "            print(lines[i].strip(), lines[i+1].strip())\n",
    "        if i==4:\n",
    "            print(\"...\")\n",
    "    print(lines[-1].strip(), lines[-2].strip())\n",
    "    return page_list, title_list\n",
    "# (p,t)=read_page_list()\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import copy\n",
    "# Load the URLs and titles\n",
    "(urls, titles)=read_page_list()\n",
    "print(\"Finished reading %d page addresses\" % len(urls))\n",
    "print(\"Now load them one by one\")\n",
    "# Load documents from the URLs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "print(\"Finished loading %d documents\" % len(docs))\n",
    "print(\"Now split them using the separator %s\" % SEP)\n",
    "\n",
    "Docs_List=[]\n",
    "for d in docs:\n",
    "    sections=sepstr(d[0].page_content)\n",
    "    isplit=0\n",
    "    print(d[0].metadata['source'], end=\"\")\n",
    "    for s in sections:\n",
    "        d2 = copy.deepcopy(d[0])  # Create a deep copy of d[0]\n",
    "        d2.page_content=s\n",
    "        d2.id=isplit\n",
    "        isplit+=1\n",
    "        # d2.metadata['source']=d.metadata['source']\n",
    "        Docs_List.append(d2)\n",
    "print(\"Finished splitting %d documents to %d\" % (len(docs), len(Docs_List)))\n",
    "#\n",
    "if vectorstore_exists():\n",
    "    Varr=load_vectorstore()\n",
    "    print(\"Loaded vectorstore\")\n",
    "else:\n",
    "    print(\"Creating vectorstore\")\n",
    "    Varr=docs2vectors(Docs_List)\n",
    "    save_vectorstore(vectorstore=Varr)\n",
    "    print(\"Created and saved vectorstore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something funny about the length of the context provided to the llm ##\n",
    "I did not really pursue this but sometimes it helps to feed the `llm` just the contents that has the requiredinformation but a long text that includes the required information but other thinsg as well.\n",
    "\n",
    "For example, for the question=\"What is the average U.S. life expectancy?\", the answer is in the first document in the list of most similar documents.  However, if I give the entire `page_content` of that document, *llama3* fails to find the answer.\n",
    "\n",
    "Try `context_length=None` to verify this.\n",
    "\n",
    "When I have `context_length=5000`, it works.  When I have it as `6000`, it does NOT work.\n",
    "\n",
    "**Llama 3.1** is supposed to have a context window length of 128K.  It looks however, its attention is diluted when the context length exceeds 5000.  This is true at least for this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query functions ##\n",
    "You can query in two modes:\n",
    "### Exact Reference ###\n",
    "Locate the post that refers to a person or to a specific object (keyword):\n",
    "* Person example \"Who is Grigory Potemkin\"\n",
    "* Keyword example \"Locate electrolyser\"\n",
    "\n",
    "\n",
    "Ths syntaxt is strict.  It should be\n",
    "* \"Who is \" + PersonName or\n",
    "* \"Locate \" + ObjectName\n",
    "\n",
    "The `Short_List` is populated with ALL the documents that comtains the required person or the keyword.   `showlist(Short_List)` prints them as a list.\n",
    "\n",
    "### Directed query ###\n",
    "* Use Short List Document x to answer Question y: \"SxQy\"\n",
    "* `S=x` All future queries will be answered using Short List document x\n",
    "* `S=None` Clear an earlier `S=x`\n",
    "* `Q+...` append '...' to `Questions`\n",
    "\n",
    "### Chatbot-like usage ###\n",
    "Any text string that does not start with \"Who is \" or \"Locate \" is treated as a genuine chatbot query.  The function `similars` finds the NSIMILAR documents closest to the query text.  The function `respond` gets the `llm` to respond by going the through the top NSIMILAR documents.\n",
    "\n",
    "`respond` starts from the closest document and moves on to the next one only if the response is \"I do not know\". The LLM is instructed to respond \"I do not know\" if it cannot retrieve the requested information from the attached document (which is atached as `context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "expanded_question=\"\"\n",
    "Short_List=[]\n",
    "Short_Sims=[]\n",
    "Questions=[]\n",
    "Refer=None\n",
    "Xpand=True\n",
    "\n",
    "def query(question, ragtext, context_length=None):\n",
    "    llm = Ollama(model=\"llama3\")\n",
    "    if ragtext==None:\n",
    "        response=llm.invoke(\"Write a paragraph to answer the following question : \"+question) \n",
    "    else:\n",
    "        prompt = \"\"\"Based ONLY on the following information, answer the question. Do not use any external knowledge. If the answer cannot be found in the provided text, say 'I do not know'\n",
    "                    Context: {context}\n",
    "\n",
    "                    Question: {question}\n",
    "\n",
    "                    Answer:\"\"\"\n",
    "        if context_length==None:\n",
    "            context=ragtext\n",
    "        else:\n",
    "            context=ragtext[0:context_length]\n",
    "    #\n",
    "        response = llm.invoke(prompt.format(context=context, question=question)) # response is a string\n",
    "    return response\n",
    "#\n",
    "\n",
    "def keyword_references(word):\n",
    "    indices=[]\n",
    "    for i in range(len(Docs_List)):\n",
    "        if word in Docs_List[i].page_content:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def showlist(lst):\n",
    "    log2file(\"\\n**Short list**\\n\\n\")\n",
    "    if len(lst)==0:\n",
    "        log2file(\"There is no short list\")\n",
    "        return\n",
    "    for idx, doc in enumerate(Short_List, start=0):\n",
    "        s=f\"* {idx}. {doc.metadata['source']}\"\n",
    "        s+=f\" Section #{doc.id}\"\n",
    "        s+=f\" --> '{doc.page_content[0:20]}'\"\n",
    "        s+=\"\\n\"\n",
    "        log2file(s)\n",
    "\n",
    "\n",
    "NSIMILAR=3\n",
    "# The following function populates Short_List[] with the most similar documents to the given question\n",
    "# It returns True to continue with the LLM, or False to show the list of similar documents\n",
    "def similars(question, n=NSIMILAR, show=False):\n",
    "    global expanded_question, Short_List, Short_Sims\n",
    "    askllm=False\n",
    "    if Xpand:\n",
    "        expanded_question=query(question, None)\n",
    "        s=(f\"Find {n} docs closest to the expanded text:\\n{expanded_question}\\n\")\n",
    "    else:\n",
    "        expanded_question=question\n",
    "        s=(f\"Find {n} docs closest to the question:\\n{question}\\n\")\n",
    "    log2file(s)\n",
    "    print(s)\n",
    "    similardoc_indices, Short_Sims = most_similar(embedstr(expanded_question), Varr, n=n)\n",
    "    askllm=True\n",
    "    #\n",
    "    Short_List = [Docs_List[i] for i in similardoc_indices]\n",
    "    if show:\n",
    "        showlist(Short_List)\n",
    "    return askllm\n",
    "\n",
    "#\n",
    "def respond(question, similardocs):\n",
    "    for i in range(len(similardocs)):\n",
    "        response= query(question, similardocs[i].page_content, None)\n",
    "        # s=similardocs[i].metadata['source']\n",
    "        # s+=f\" Section #{similardocs[i].id}\"\n",
    "        # s+=f\" --> '{response[0:14]}'\"\n",
    "        # s+=\"\\n\\n\"\n",
    "        log2file(f\"{i}. `{response}`\\n\")\n",
    "        if 'I do not know' not in response:\n",
    "            return response\n",
    "    return \"\"\n",
    "# The followimg function returns the summary of the page_content of a document\n",
    "def summarise(doc):\n",
    "    log2file(\"\\nSummarise \"+doc.metadata['source']+\" #\"+str(doc.id)+\"\\n\\n\")\n",
    "    llm = Ollama(model=\"llama3\")\n",
    "    response = llm.invoke(\"Please summarise the following text: \"+doc.page_content)\n",
    "    log2file(response)\n",
    "    return response\n",
    "\n",
    "# The following function checks if the string argument is in the list Questions[].\n",
    "# If it is not, it adds it to the Questions[] list.\n",
    "def addquestion(question):\n",
    "    if question not in Questions:\n",
    "        Questions.append(question)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# The following function reads text from a file, creates a Doc object with page_content being the read text\n",
    "# and appends it to Short_List\n",
    "def read2short(file_name):\n",
    "    global Short_List\n",
    "    with open(f\"data/{file_name}.txt\", 'r') as f:\n",
    "        text = f.read()\n",
    "    d2 = copy.deepcopy(Short_List[-1])  # Create a deep copy of d[0]\n",
    "    d2.page_content=text\n",
    "    d2.id=0\n",
    "    d2.metadata['source']=file_name+'.txt'\n",
    "    Short_List.append(d2)\n",
    "\n",
    "# String s is of the form \"xxx abcde\" where xxxx is an integer and abcde is a string\n",
    "# The function returns the integer xxxx and the string abcde\n",
    "def getiands(s):\n",
    "    i=s.find(\" \")\n",
    "    if i==-1:\n",
    "        return None, None\n",
    "    return int(s[0:i]), s[i+1:]\n",
    "\n",
    "# Check if the argument string is of the form \"SxxxQyyy\" where xxx and yyy are integers\n",
    "# and return the values of xxx and yyy\n",
    "def getindexes(s):\n",
    "    if s[0]==\"S\":\n",
    "        s=s[1:]\n",
    "    if s[0]==\"Q\":\n",
    "        s=s[1:]\n",
    "    i=s.find(\"Q\")\n",
    "    if i==-1:\n",
    "        return None, None\n",
    "    return int(s[0:i]), int(s[i+1:])\n",
    "# print(getindexes(\"S123Q456\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User interface ##\n",
    "The user interface is not fancy.  I use the python `input()` function.  This appears as a window at the top of the notebook window in VS Code.\n",
    "\n",
    "|Command|What it does|\n",
    "|:---:|-----------|\n",
    "|blank line|_Quit_|\n",
    "|!ASxQy|_Answer the question #y using the short listed doc #x_|\n",
    "|!AQy|_Answer question #y based on the Document #x of last Sx command_|\n",
    "|!CSx|_Condense short-list doc #x_|\n",
    "|!Ftext|_Find the documents that contain text_|\n",
    "|!PS|_List all short-listed documents_|\n",
    "|!PQ|_List all past questions_|\n",
    "|!RS file_name|_Read `file_name`.txt into a document, and append to Short_List|\n",
    "|!SSx file_name|_Save Short List Document #x to `file_name`.txt_|\n",
    "|!+Qtext|_Add text as a new question to `Questions`_|\n",
    "|!XT|_Expand questin (stop expanding if XF)_|\n",
    "|text|_Find the most similar NSIMILAR documents and try to answer_|\n",
    "\n",
    "The following cell gets the user input and returns:\n",
    "* `Q` for quit of the entry is blank\n",
    "* Lists the short list documents and returns `None` if the entry is **Shorts** or **shorts**\n",
    "* List past questions if the entry is **questions** or **Questions**\n",
    "* Summarise the 5th short list document if the input is \"5\" (or any other digit)\n",
    "* Use Short List Document x to answer Question y: \"SxQy\"\n",
    "* Returns with the entry text for any other user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xqtcommand(q):\n",
    "    global Refer, Short_List\n",
    "    # q=q.upper()\n",
    "    if q[0]=='P':\n",
    "        if q[1]=='S':\n",
    "            showlist(Short_List)\n",
    "        elif q[1]=='Q':\n",
    "            for i, q in enumerate(Questions, start=0):\n",
    "                log2file(f\"{i:03d}. {q}\"+\"\\n\\n\")\n",
    "    elif q[0]=='A':\n",
    "        if q[1]=='S':\n",
    "            [x,y]=getindexes(q[1:])\n",
    "            if x!=None and y!=None:\n",
    "                s=f\"Answer question {y} in the short-listed document {x}\"\n",
    "                log2file(s+\"\\n\\n\")\n",
    "                if x>=0 and x<len(Short_List):\n",
    "                    response=respond(Questions[y], [Short_List[x]])\n",
    "                    log2file(response+\"\\n\\n\")\n",
    "                    Refer=x\n",
    "                else:\n",
    "                    log2file(\"Index out of range\")\n",
    "        elif q[1]=='Q':\n",
    "            y=int(q[2:])\n",
    "            if y>=0 and y<len(Questions):\n",
    "                response=respond(Questions[y], [Short_List[Refer]])\n",
    "            else:\n",
    "                response=\"Question index out of range\"\n",
    "            log2file(response+\"\\n\\n\")\n",
    "    elif q[0]=='S':\n",
    "        if q[1]=='S':\n",
    "            [x,filename]=getiands(q[2:])\n",
    "            if x!=None and filename!=None:\n",
    "                with open(f\"data/{filename}.txt\", 'w') as file:\n",
    "                    file.write(Short_List[x].page_content)\n",
    "    elif q[0]=='+':\n",
    "        if q[1]=='Q':\n",
    "            addquestion(q[2:]) \n",
    "    elif q[0]=='C':\n",
    "        if q[1]=='S':\n",
    "            Refer=int(q[2:])\n",
    "            if Refer>=0 or Refer<len(Short_List):\n",
    "                summarise(Short_List[Refer])\n",
    "            else:\n",
    "                log2file(\"Index out of range\")\n",
    "    elif q[0]=='F':\n",
    "        similardoc_indices=keyword_references(q[1:])\n",
    "        Short_List = [Docs_List[i] for i in similardoc_indices]\n",
    "        showlist(Short_List)\n",
    "    elif q[0]=='R':\n",
    "        if q[1]=='S':\n",
    "            s=q[2:].lstrip()\n",
    "            read2short(s)\n",
    "    elif q[0]=='X':\n",
    "        global Xpand\n",
    "        if q[1]=='T':\n",
    "            Xpand=True\n",
    "        else:\n",
    "            Xpand=False\n",
    "            \n",
    "                \n",
    "\n",
    "def getquery():\n",
    "    global Refer\n",
    "    log2file(\"---\")\n",
    "    s=f\"X={Xpand}; \"\n",
    "    if Refer!=None:\n",
    "        s+=f\"Ref={Refer}. \"\n",
    "    s+= \"Input :\"\n",
    "    print(s, end=\"\")\n",
    "    question=input(\"\")\n",
    "    print(question)\n",
    "    s+=question+\"\\n\"\n",
    "    log2file(s)\n",
    "    if question==\"\":\n",
    "        return 'Q'\n",
    "    if question[0]==\"!\":\n",
    "        xqtcommand(question[1:])\n",
    "        return None\n",
    "    addquestion(question)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep calling `getquery` until the user enters a blank string ##\n",
    "The following keeps calling `getquery` in a loop.  The output keeps adding up so you have access to the entire chat. The previous questions are not entered.  So this is not a true chat.  It is relatively easy to add a memory and add the previous questions and the LLM answer to the context for the new question but I need to get more familiar with the way LLM works before I do that and therefore I keep it simple at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=True; Input :What are typical electrolyser efficiencies?\n",
      "Question : What are typical electrolyser efficiencies?Find 3 docs closest to the expanded text:\n",
      "Typical electrolyzer efficiencies, also known as faradaic efficiency or current efficiency, vary depending on the type of electrolyzer and its specific design. In general, most alkaline electrolyzers have efficiencies ranging from 70% to 85%, while proton exchange membrane (PEM) electrolyzers typically achieve efficiencies between 80% and 90%. Solid oxide electrolyzers can reach efficiencies as high as 95% or more, although these are still relatively rare and typically used in specialized applications. Advanced technologies like high-temperature electrolyzers and liquid metal-based electrolyzers have also demonstrated efficiencies above 90%, but these are still in the early stages of development. Overall, while there is a range of efficiencies depending on the specific technology, most commercial-scale electrolyzers operate at efficiencies between 75% to 85%.\n",
      "\n",
      "\n",
      "X=True; Input :!Felectrolyser\n",
      "X=True; Input :!AS0Q0\n",
      "X=True; Ref=0. Input :What is Fermi Paradox?\n",
      "Question : What is Fermi Paradox?Find 3 docs closest to the expanded text:\n",
      "The Fermi Paradox, named after physicist Enrico Fermi, questions why we have not yet observed any signs of intelligent extraterrestrial life despite the high probability of its existence. In 1950, Fermi famously asked \"Where is everybody?\" during a lunchtime conversation at Los Alamos National Laboratory, sparking the paradox's investigation. The paradox arises from the apparent contradiction between the likelihood of the existence of other civilizations in the universe and our lack of evidence or contact with them. With an estimated 10-20 billion stars likely to have planets that could potentially support life, the probability of intelligent life emerging elsewhere seems high. Yet, despite centuries of searching, we have found no definitive proof of extraterrestrial life, leading to questions about why we have not yet detected any signs of life, such as radio signals or other technological transmissions. The Fermi Paradox has led to ongoing discussions and speculation about the possibility of intelligent life existing elsewhere in the universe.\n",
      "\n",
      "\n",
      "X=True; Ref=0. Input :What is the cost of sending 1 kg to the orbit?\n",
      "Question : What is the cost of sending 1 kg to the orbit?Find 3 docs closest to the expanded text:\n",
      "The cost of sending 1 kilogram (or 2.2 pounds) of payload to orbit can vary greatly depending on several factors, including the specific orbit desired, the type of launch vehicle used, and the level of technology employed. Generally speaking, a rough estimate for the cost of launching 1 kg of payload to Low Earth Orbit (LEO), which is approximately 200-800 kilometers above the Earth's surface, can range from $20,000 to $50,000 or more per kilogram, depending on the launch provider and the specific mission requirements. For example, a popular commercial launch service like SpaceX's Falcon 9 may charge around $35,000 to $40,000 per kilogram for LEO insertion, while a government-funded space agency or a research organization may be able to secure a lower rate through contracts and agreements. In contrast, launching 1 kg of payload to Geostationary Transfer Orbit (GTO), which is around 36,000 kilometers above the equator, can cost anywhere from $100,000 to $500,000 or more per kilogram, due to the higher energy requirements and longer trajectory necessary for achieving this orbit.\n",
      "\n",
      "\n",
      "X=True; Ref=0. Input :!XF\n",
      "X=False; Ref=0. Input :What are typical electrolyser efficiencies?\n",
      "Question : What are typical electrolyser efficiencies?Find 3 docs closest to the question:\n",
      "What are typical electrolyser efficiencies?\n",
      "\n",
      "\n",
      "X=False; Ref=0. Input :What is Fermi Paradox?\n",
      "Question : What is Fermi Paradox?Find 3 docs closest to the question:\n",
      "What is Fermi Paradox?\n",
      "\n",
      "\n",
      "X=False; Ref=0. Input :\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open the log file\n",
    "open_log()\n",
    "question=getquery()\n",
    "while question!=\"Q\":\n",
    "    if question is not None:\n",
    "        print(\"Question : \" + question, end=\"\")\n",
    "        askllm = similars(question, show=True)\n",
    "        if askllm:\n",
    "            response=respond(question, Short_List)\n",
    "            # log2file(response)\n",
    "            print(response)\n",
    "        else:\n",
    "            print(\"n/a\")\n",
    "    question=getquery()\n",
    "close_log()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
